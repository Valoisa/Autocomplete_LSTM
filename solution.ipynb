{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02ad7bf",
   "metadata": {},
   "source": [
    "### Загружаем данные для обучения: `train_texts`, `val_texts`, `test_texts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee18919",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\YandexDL\\Sprint2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from os.path import isfile\n",
    "\n",
    "from src.data_utils import clean_text\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "train_texts, val_texts, test_texts = None, None, None\n",
    "foldername = 'data'\n",
    "raw_dataset_filepath = os.path.join(foldername, 'raw_dataset.csv')\n",
    "processed_dataset_filepath = os.path.join(foldername, 'processed_dataset.csv')\n",
    "train_filepath = os.path.join(foldername, 'train.csv')\n",
    "val_filepath = os.path.join(foldername, 'val.csv')\n",
    "test_filepath = os.path.join(foldername, 'test.csv')\n",
    "text_column = 'text'\n",
    "\n",
    "# Если такие файлы уже есть, загружаем данные для обучения из них\n",
    "if isfile(train_filepath) and isfile(val_filepath) and isfile(test_filepath):\n",
    "    train_texts = pd.read_csv(train_filepath).dropna()[text_column].to_list()\n",
    "    val_texts = pd.read_csv(val_filepath).dropna()[text_column].to_list()\n",
    "    test_texts = pd.read_csv(test_filepath).dropna()[text_column].to_list()\n",
    "# В противном случае скачиваем датасет, обрабатываем, делим его на выборки и сохраняем\n",
    "else:\n",
    "    dataset = load_dataset('sentiment140', trust_remote_code=True, split='train', cache_dir='data')['text']\n",
    "    dataset_df = pd.DataFrame({ text_column: dataset })\n",
    "    dataset_df.to_csv(raw_dataset_filepath, index=False)\n",
    "\n",
    "    texts = list(map(clean_text, dataset))\n",
    "    print(len(texts))\n",
    "    texts_df = pd.DataFrame({ text_column: texts })\n",
    "    texts_df.to_csv(processed_dataset_filepath, index=False)\n",
    "\n",
    "    val_test = 0.2\n",
    "    test = 0.5\n",
    "    train_texts, val_test_texts = train_test_split(texts, test_size=val_test, random_state=42)\n",
    "    val_texts, test_texts = train_test_split(val_test_texts, test_size=test)\n",
    "\n",
    "    train_df = pd.DataFrame({ text_column: train_texts })\n",
    "    train_df.to_csv(train_filepath, index=False)\n",
    "\n",
    "    test_df = pd.DataFrame({ text_column: test_texts })\n",
    "    test_df.to_csv(test_filepath, index=False)\n",
    "\n",
    "    val_df = pd.DataFrame({ text_column: val_texts })\n",
    "    val_df.to_csv(val_filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb5463",
   "metadata": {},
   "source": [
    "### Из полученных данных формируем датасеты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eec5cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token added: [EOS] 30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.next_token_dataset import NextTokenDataset, EvalROUGEDataset, collate_fn\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "# Добавим в токенайзер признак конца строки\n",
    "tokenizer.add_special_tokens({'eos_token': '[EOS]'})\n",
    "print('EOS token added:', tokenizer.eos_token, tokenizer.eos_token_id)\n",
    "\n",
    "train_dataset = NextTokenDataset(train_texts, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = NextTokenDataset(val_texts, tokenizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "# Датасет для вычисления метрик ROUGE на валидационных данных\n",
    "val_rouge_dataset = EvalROUGEDataset(val_texts, tokenizer)\n",
    "\n",
    "test_dataset = NextTokenDataset(test_texts, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a603fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lstm_model import LSTMAutoComplete\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Определяем модель:\n",
    "## 4 скрытых слоя\n",
    "## размерность скрытого состояния 128\n",
    "model = LSTMAutoComplete(len(tokenizer.get_vocab()), hidden_size=128, num_layers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d53559",
   "metadata": {},
   "source": [
    "### Обучаем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af37335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 106/106 [00:10<00:00,  9.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 10.326239720830378\n",
      "Rouge metrics:\n",
      "rouge1: 0.0002\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0002\n",
      "rougeLsum: 0.0001\n",
      "well my friend branching distorted lawyerspipe nellie continued owes × darkeningdoor promotingrove started rebelsetched boxingbolic\n",
      "===================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 423/423 [04:47<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, training loss: 10.1672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 106/106 [00:10<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 10.165345524841884\n",
      "Rouge metrics:\n",
      "rouge1: 0.0003\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0003\n",
      "rougeLsum: 0.0003\n",
      "well my friend nascar vector sighting mammal කgram classrooms concurrency booth callerᅧ pastortree csilithic vicious vibrating\n",
      "===================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 423/423 [04:43<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, training loss: 10.1642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 106/106 [00:10<00:00, 10.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 10.165345524841884\n",
      "Rouge metrics:\n",
      "rouge1: 0.0004\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0004\n",
      "rougeLsum: 0.0004\n",
      "well my friend teachings hyper eurasian unanimously pajamas nowadays advocacy ported stairsio opened prom lebanon soviets mouth terrified wealth\n",
      "===================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 423/423 [04:43<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, training loss: 10.1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 106/106 [00:09<00:00, 10.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 10.165345524841884\n",
      "Rouge metrics:\n",
      "rouge1: 0.0002\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0002\n",
      "rougeLsum: 0.0002\n",
      "well my friend jew silly explorer© voivodeship dinamo willy browne clarityber nouvelle [unused841]ף cages bottomsₛ stevie\n",
      "===================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 423/423 [04:45<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, training loss: 10.1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 106/106 [00:09<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 10.165345524841884\n",
      "Rouge metrics:\n",
      "rouge1: 0.0003\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0003\n",
      "rougeLsum: 0.0003\n",
      "well my friend boone broncos garden principality relate martian mazdailingnder extremely debbie preoccupied cluster morality matched ᆷ 健\n",
      "===================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 5: 100%|██████████| 423/423 [04:09<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, training loss: 10.1643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 106/106 [00:08<00:00, 12.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs+\u001b[32m1\u001b[39m):\n\u001b[32m     37\u001b[39m     train_loss = train_one_epoch(model, device, epoch, train_dataloader, optimizer, criterion, scheduler)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     val_loss = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_rouge_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_phrase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_phrase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m===================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     40\u001b[39m     train_losses.append(train_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\YandexDL\\Sprint2\\src\\model_eval.py:49\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, tokenizer, device, val_dataloader, val_rouge_dataset, criterion, test_phrase)\u001b[39m\n\u001b[32m     47\u001b[39m     max_len = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m) * \u001b[32m4\u001b[39m // \u001b[32m3\u001b[39m\n\u001b[32m     48\u001b[39m     input_ids = torch.tensor(tokenizer.encode(\u001b[38;5;28minput\u001b[39m, add_special_tokens=\u001b[38;5;28;01mFalse\u001b[39;00m), dtype=torch.long)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     model_out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     generated.append(tokenizer.decode(model_out, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m     51\u001b[39m rouge_res = rouge.compute(predictions=generated, references=references)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mx:\\YandexDL\\Sprint2\\src\\lstm_model.py:45\u001b[39m, in \u001b[36mLSTMAutoComplete.generate\u001b[39m\u001b[34m(self, input, eos_token, max_len, temperature)\u001b[39m\n\u001b[32m     43\u001b[39m next_token_logits = logits[-\u001b[32m1\u001b[39m, :] / temperature\n\u001b[32m     44\u001b[39m probs = softmax(next_token_logits, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m next_token = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_token.item() == eos_token:\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from src.lstm_model import LSTMAutoComplete\n",
    "from src.model_train import train_one_epoch\n",
    "from src.model_eval import evaluate\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Определяем модель:\n",
    "## 4 скрытых слоя\n",
    "## размерность скрытого состояния 128\n",
    "model = LSTMAutoComplete(len(tokenizer.get_vocab()), hidden_size=128, num_layers=4)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "scheduler = StepLR(optimizer, 5, 0.1)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "epochs = 20\n",
    "test_phrase = \"what are you talking\"\n",
    "\n",
    "# Посмотрим на валидационные метрики до обучения:\n",
    "print('=== Before training ===')\n",
    "evaluate(model, tokenizer, device, val_dataloader, val_rouge_dataset, criterion, test_phrase=test_phrase)\n",
    "print('===================================\\n')\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Обучаем модель в течение 20-ти эпох\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss = train_one_epoch(model, device, epoch, train_dataloader, optimizer, criterion, scheduler)\n",
    "    val_loss = evaluate(model, tokenizer, device, val_dataloader, val_rouge_dataset, criterion, test_phrase=test_phrase)\n",
    "    print('===================================\\n')\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "plt.plot(list(range(1, epochs+1)), train_losses, color='b')\n",
    "plt.plot(list(range(1, epochs+1)), val_losses, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c833c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.pipelines.text_generation.TextGenerationPipeline'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "print(type(generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64bce181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.autograd.grad_mode.no_grad'>\n"
     ]
    }
   ],
   "source": [
    "print(generator.get_inference_context())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f507b4",
   "metadata": {},
   "source": [
    "### Теперь на тестовых данных сравним работу LSTM модели и GPT2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7183022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Autocompleting inputs: 100%|██████████| 10/10 [00:06<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge metrics for test (GPT2):\n",
      "rouge1: 0.0337\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0337\n",
      "rougeLsum: 0.0333\n",
      "Rouge metrics for test (LSTM-based):\n",
      "rouge1: 0.0000\n",
      "rouge2: 0.0000\n",
      "rougeL: 0.0000\n",
      "rougeLsum: 0.0000\n",
      "=== Samples ===\n",
      "Input: \"i cant believe that i am still waiting for a damn\"\n",
      "\tGPT2-autocomplete: \" good time.\"\n",
      "\tLSTM-autocomplete: \"rebelled toxic silently locomotives sliding gunmen cross engaging convoys supporters [unused361] zoo\"\n",
      "===================================\n",
      "\n",
      "Input: \"tchat with jake lmfao its frikkin awesome i miss this effin boy so much\"\n",
      "\tGPT2-autocomplete: \" i love it i love it im going to be a guy i love it i love it i love it\"\n",
      "\tLSTM-autocomplete: \"tori achievements incredible authority improper meath← obituary airessters yingupt tiger internship remarked humble cortex slack pains suspicion announce\"\n",
      "===================================\n",
      "\n",
      "Input: \"i find my bk in the fd bks section and n the pho section because\"\n",
      "\tGPT2-autocomplete: \" that way it's not a bk anymore and i just want to get rid of it\"\n",
      "\tLSTM-autocomplete: \"smashed warehouse archival kia yoga bulgarian canal himalayas sweetie dared marketing hazel reducesay compliant easier effects until\"\n",
      "===================================\n",
      "\n",
      "Input: \"im watching house and i need\"\n",
      "\tGPT2-autocomplete: \" to get out of my car\"\n",
      "\tLSTM-autocomplete: \"zombie wipe river mustache weakly lenny\"\n",
      "===================================\n",
      "\n",
      "Input: \"omg you are starting be my magazines i havent seen your vids\"\n",
      "\tGPT2-autocomplete: \" i am going to give your magazine a chance to be more interesting i think\"\n",
      "\tLSTM-autocomplete: \"opinion¼ replacementsbeaticleslind voice 320 slick の gerald mirrored charlotte maintenance swift\"\n",
      "===================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.gpt_vs_lstm_eval import evaluate_gpt_vs_lstm\n",
    "\n",
    "evaluate_gpt_vs_lstm(model, tokenizer, test_texts, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
